# -*- coding: utf-8 -*-
"""sentiment-international .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YwPOT92IHSlhH0RD-Sgzg86y2_WHZl8y
"""

pip install pyspark

import pandas as pd
pd.set_option('display.max_columns',100, 'display.max_colwidth',1000, 'display.max_rows',1000, 
              'display.float_format', lambda x: '%.2f' % x)
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import sparknlp
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .master("local[4]")\
    .config("spark.driver.memory","4G")\
    .config("spark.driver.maxResultSize", "2G") \
    .config("spark.jars.packages", "com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5")\
    .config("spark.kryoserializer.buffer.max", "1G")\
    .getOrCreate()
from sparknlp.pretrained import PretrainedPipeline
from pyspark.sql.functions import from_unixtime, to_date, year, udf, explode, split, col, length, rank, dense_rank, avg, sum
from pyspark.sql.window import Window
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.stat import Correlation
from pyspark.ml.clustering import BisectingKMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml import Pipeline
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

import nltk
from nltk.corpus import stopwords
from collections import Counter
from wordcloud import WordCloud

from textblob import TextBlob

"""### Load / Cache Data
- Spark dataframe should split into partitions = 2-3x the no. threads available in your CPU or cluster. I have 2 cores, with 2 threads each = 4, and I chose 3x, ie. 12 partitions, based on experimentation.
- Then cache tables: you can see in Spark GUI that 12 partitions are cached for each file.
- The Shuffle Read is default to 200, we don't want this to be the bottleneck, so we set this equal to partitions in our data, using spark.sql.shuffle.partitions. This is specific to wide shuffle transformations (e.g. GROUP BY or ORDER BY) that may be performed later on, and how many partitions this operation sets up to read the data.
"""

tweets = spark.read.csv(r"/content/intlstud.csv", header=True)
users = spark.read.csv(r"/content/intluserdata_csv.csv", header=True)

spark.sql('SET spark.sql.shuffle.partitions=12')

"""## EDA

### Users data
"""

#change the Spark SQL DataFrame column type from one data type to another data 
users = users.withColumn("Followers",col("Followers").cast("long"))
users = users.withColumn("Following",col("Following").cast("long"))
users = users.withColumn("Favorites",col("Favorites").cast("long"))

users.dropna()

users.printSchema()

users.createOrReplaceTempView('users')
spark.sql('SELECT * FROM users LIMIT 20').toPandas()

"""Select columns I will need from users file, and add rank columns I will use later:"""

users_verified = users.select('User Id','ScreenName','Location','Favorites',
                              'Followers','Following','Verified') \
.filter('Verified == "TRUE"')

users_verified = users_verified.withColumn("followers_rank",
                                           dense_rank().over(Window.orderBy(col("Followers").desc())))\
                                .withColumn("friends_rank",
                                           dense_rank().over(Window.orderBy(col("Following").desc())))\
                                .withColumnRenamed('User Id','id_user')

query = '''
SELECT ScreenName, Followers
FROM users
ORDER BY Followers DESC
'''
spark.sql(query).show()

query = '''
SELECT ScreenName, Following
FROM users
ORDER BY Following DESC
'''
spark.sql(query).show()

query = '''
SELECT name, Favorites
FROM users
ORDER BY Favorites DESC
'''
spark.sql(query).show()

print('Total no. verified users on twitter:', users.filter('Verified == "TRUE"').count())
print("\n Below we see there are a few members with a large following. These could be influential in pushing the international student's cause \n")

fig, ax = plt.subplots(ncols=3, figsize=(14,4))
users.select('Followers').toPandas().hist(bins=20, ax=ax[0])
users.select('Following').toPandas().hist(bins=20, ax=ax[1])
users.select('Favorites').toPandas().hist(bins=20, ax=ax[2]);

"""### Tweets data"""

tweets = tweets.withColumn("Favorites",col("Favorites").cast("int"))
tweets = tweets.withColumn("Retweets",col("Retweets").cast("int"))
tweets = tweets.withColumn("Hashtags",col("Hashtags").cast("int"))
tweets = tweets.withColumn("Mentions",col("Mentions").cast("int"))

tweets.printSchema()

tweets.createOrReplaceTempView('tweets')
spark.sql('SELECT * FROM tweets LIMIT 2').toPandas()

query = '''
SELECT tweets.Mentions AS Mentions, COUNT(*) as cnt
FROM tweets
GROUP BY Mentions
ORDER BY cnt DESC
'''
spark.sql(query).show()

query = '''
SELECT tweets.Hashtags AS Hashtags, COUNT(*) as cnt
FROM tweets
GROUP BY Hashtags
ORDER BY cnt DESC
'''
spark.sql(query).show()

"""## Bag of Words
- Interesting to see what themes were most discussed in each year, and by whom.
- Most tweets didn't contain hashtags, so in order to widen the field, I will need to use a Word Vectorizer to parse out the key themes across all tweets.
- I will use NLTK's list of stopwords to conduct stopword removal, and visualise results in wordclouds
"""

import nltk
nltk.download('stopwords')

def stopwords_delete(word_list):
        from nltk.corpus import stopwords
        filtered_words=[]
        print (word_list)

def remove_stopwords(x):    
    sw = stopwords.words("english")
    string = ''
    for x in x.split(' '):
        if x.lower() not in sw:
            string += x + ' '
        else:
            pass
    return string

nosw = udf(remove_stopwords)
spark.udf.register("nosw", nosw)
tweets = tweets.withColumn('text_nosw',nosw('text'))

"""### Sentiment Progression
Below we will chart the progression of sentiment and volume of tweets over time, and repeat this for specific themes
"""

tweets_sent = pipeline.transform(tweets.select('id','text','created_at','sentiment','user_id'))\
                            .select('id','text','created_at',col('entities.result').alias('entities'),col('pos.result').alias('pos'),'sentiment','user_id')\
                            .withColumn('entities',make_string('entities'))\
                            .withColumn('pos',make_string('pos'))

def plot_trend(chart_title, with_filter=False, filter_string=None, sentiment_sdf=tweets_sent, rolling_window=90):
    ''' Plot trend in sentiment and number of tweets, with the ability to customise several features:
    
    Args:
    chart_title: str. Customise chart title to clarify what you're plotting to the reader
    with_filter: boolean. if set to True, will require filter_string to be populated. if set to False, will 
                            allow you to plot trend without filters.
    filter_string: str. filter string that should be passed into Spark Dataframe API's .filter()
    sentiment_sdf: Spark Dataframe. Input dataframe.
    rolling_window: int. Set to number of lookback days to use in calculating moving average
    
    Returns:
    Moving average plot of num_tweets and sentiment, based on any filtering applied.
    
    '''
    
    if with_filter:
        trend_pdf = sentiment_sdf.select(to_date(from_unixtime(col('created_at'))).alias('date'),
                                   'sentiment','id')\
                            .filter(str(filter_string))\
                            .groupby('date').agg({'sentiment':'avg','id':'count'})\
                            .orderBy('date')\
                            .withColumnRenamed('avg(sentiment)','sentiment')\
                            .withColumnRenamed('count(id)','num_tweets')\
                            .toPandas()

    else:
        trend_pdf = sentiment_sdf.select(to_date(from_unixtime(col('created_at'))).alias('date'),
                                   'sentiment','id')\
                            .groupby('date').agg({'sentiment':'avg','id':'count'})\
                            .orderBy('date')\
                            .withColumnRenamed('avg(sentiment)','sentiment')\
                            .withColumnRenamed('count(id)','num_tweets')\
                            .toPandas()
        
    rolling_sentiment = trend_pdf.sentiment.rolling(window=rolling_window).mean()
    rolling_tweets = trend_pdf.num_tweets.rolling(window=rolling_window).mean()

    fig, ax = plt.subplots(figsize=(10,4), sharex=True)
    fig.suptitle('{}'.format(chart_title))
    ax0 = plt.subplot(111)
    ax1 = ax0.twinx()

    ax0.plot(trend_pdf.date, rolling_sentiment, label='sentiment_ma')
    ax0.legend(loc='upper left')
    ax1.plot(trend_pdf.date, rolling_tweets, label='tweets_ma', c='k')
    ax1.legend(loc='lower right');

plot_trend('Overall Trend')